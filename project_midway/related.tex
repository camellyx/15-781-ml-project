\section{Related Works}
\label{sec:related}

We summarize three recent works that explain and explore three mechanisms to
introduce noise in multi-layer neural networks (mlp). We believe our work is
much more comprehensive than these works and we will compare with them in our
final report.

{\bf Dropout:}
Dropout proposes to regularize fully connected neural networks by
probabilistically dropping an output (set to zero) of a hidden layer
neuron~\cite{hinton2012improving} (i.e., with a low probability $(1-p)$, one of
the output of a hidden layer neutron is set to $0$ in the forward propagation
process).  This can effectively decrease test error rates by preventing
over-fitting of the model.

{\bf DropConnect:}
Inspired by Dropout~\cite{hinton2012improving}, DropConnect proposes to
probabilistically drop a weight of a hidden layer neuron (as opposed to an
output of a hidden layer neuron in DropConnect)~\cite{wan2013dropconnect}.

{\bf Maxout:}
Maxout extends Dropout and DropConnect by probabilistically set an output or a
weight of a hidden layer neuron to maximum value~\cite{goodfellow13maxout}.

