\begin{abstract}

Recent works have shown that, by allowing some inaccuracy when training deep
neural networks, not only the training performance but also the accuracy of the
model can be improved. Our work, taking those previous works as examples and
guidance, tries to study the impact of introducing different types of noise in
different components of training a deep neural network. We intend to experiment
with noise types which include Bernoulli noise, Gaussian noise, uniform random
noise, etc. We also intend to study the effects of noise in different parts of
the model which include neurons and network links in the input, hidden, and
output layers, as well as matrix multiplication and gradient computation in the
backward propagation process. Our preliminary results show that ...

\end{abstract}

